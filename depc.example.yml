SQLALCHEMY_DATABASE_URI: postgresql://user:pass@host/database?connect_timeout=5&keepalives=1&keepalives_idle=5&keepalives_interval=5&keepalives_count=2

SECRET: mysecretkey

DB_ENCRYPTION_KEY: mydbencryptionkey

BASE_UI_URL: http://127.0.0.1/

FLOAT_DECIMAL: 3

MAX_WORST_ITEMS: 15

FORCE_INSECURE_ADMIN: false

NEO4J:
    url: http://127.0.0.1:7474
    uri: bolt://127.0.0.1:7687
    username: neo4j
    password: p4ssw0rd

WARP10:
    url: https://example.com/api/v0
    rotoken: token
    wtoken: token

WARP10_CACHE:
    url: https://example.com/api/v0
    rotoken: token
    wtoken: token

REDIS_CACHE:
    url: redis://localhost/3

# Required to run DepC DAGs with Airflow
REDIS_SCHEDULER_CACHE:
    url: redis://localhost/4

BEAMIUM:
    source-dir: /opt/beamium/sources

# Please read the Loguru official documentation for further details about the logging format
LOGGING:
    level: INFO
    format: <green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | <level>{level:<8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>
    # Enable the Graylog Extended Log Format
    gelf: false

# Configure the Kafka consumer to populate the Neo4j graph database
# The consumer uses SASL/PLAIN authentication
# One topic per team
CONSUMER:
    kafka:
        hosts: localhost:9093
        batch_size: 10
        topics: [depc.my_topic]
        username: depc.consumer
        password: p4ssw0rd
        client_id: depc.consumer
        group_id: depc.consumer.depc_consumer_group
    heartbeat:
        delay: 5

# Optional: exclude a specific team or rule from the regular QoS compute method
# This variable needs a list of UUIDs used in the database
# The compute for these IDs will be based only on the data points sent by the source
EXCLUDE_FROM_AUTO_FILL:
    - 123e4567-e89b-12d3-a456-426655440000
